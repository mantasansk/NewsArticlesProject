{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc79dd5",
   "metadata": {},
   "source": [
    "##### !! Web scraping requires firefox browser to be installed !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760729b4",
   "metadata": {},
   "source": [
    "# Gathering Data from lrt.lt and 15min.lt Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adb10c",
   "metadata": {},
   "source": [
    "## lrt.lt Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing from which categories to gather the data\n",
    "categories = [\"lietuvoje\", \"nuomones\", \"pasaulyje\", \"verslas\", \"lrt-tyrimai\", \"eismas\",\n",
    "              \"sportas\", \"pozicija\", \"sveikata\", \"mokslas-ir-it\", \"kultura\", \"veidai\",\n",
    "              \"gyvenimas\", \"tavo-lrt\", \"muzika\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ff343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting fields of interest for every article\n",
    "news_dict = {\"source\": [], \"category\": [], \"date\": [], \"title\": [], \"score\": [], \"last_updated\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.date.strftime(datetime.date.today(), format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for removing the 'Cookies' window\n",
    "cookies_accepted = False\n",
    "warmup_time = 15\n",
    "\n",
    "# Selecting maximum number of pages with articles for browser to open for each category\n",
    "max_pages = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting up headless browser as driver\n",
    "\n",
    "# geckodriver.exe is used to interact with the headless browser\n",
    "geckodriver_path = \"geckodriver.exe\"\n",
    "\n",
    "service = Service(geckodriver_path)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options, service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57217676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering information of articles for every category in categories list\n",
    "for category in categories:\n",
    "    # Articles for each category can reached by 'https://www.lrt.lt/naujienos/CATEGORY' where CATEGORY is the name of category\n",
    "    url = f\"https://www.lrt.lt/naujienos/{category}\"\n",
    "    page_num = 1\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    # Accepting cookies if needed, because 'Cookies' window blocks the necessary button\n",
    "    while not cookies_accepted:\n",
    "        try:\n",
    "            driver.find_element(by=By.CSS_SELECTOR, value=\"#CybotCookiebotDialogBodyButtonAccept\").click()\n",
    "            cookies_accepted = True\n",
    "            print(\"Cookies successfully accepted!\")\n",
    "        except:\n",
    "            if warmup_time <= 0:\n",
    "                cookies_accepted = True\n",
    "                print(\"Warmup time ended, no cookies this time!\")\n",
    "            warmup_time -= 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Loading additional pages of articles if possible\n",
    "    while page_num <= max_pages:\n",
    "        print(f\"{category}. Getting page number: {page_num}\")\n",
    "        try:\n",
    "            element = driver.find_element(by=By.CSS_SELECTOR, value='[onclick=\"_load_more(this)\"]').click()\n",
    "        except:\n",
    "            page_num = max_pages+1\n",
    "            print(f\"Not enough pages in {category} category!\")\n",
    "        page_num += 1\n",
    "        time.sleep(0.005)\n",
    "    \n",
    "    # Converting whole page to html and then parsing it with BeautifulSoup\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Iterating through acrticles and collecting desired information\n",
    "    for news in soup.find(id=\"category_list\").find_all(class_=\"col\"):\n",
    "        # Getting article's title\n",
    "        title_link = news.findChild(\"a\").attrs[\"href\"].split(\"/\")[-1]\n",
    "        title = title_link.replace(\"-\", \" \")\n",
    "\n",
    "        # Getting article's publishment date\n",
    "        pub_dt = news.findChild(class_=\"info-block__text\").text\n",
    "        pub_date = pub_dt.split()[0]\n",
    "\n",
    "        # Getting article's 'facebook likes' collected so far as the score\n",
    "        if news.findChild(class_=\"btn btn--primary btn--xs btn--fb\"):\n",
    "            fb_button = news.findChild(class_=\"btn btn--primary btn--xs btn--fb\")\n",
    "            fb_score = fb_button.findChild(class_=\"btn__text text-purple-2\").text\n",
    "        else:\n",
    "            fb_score = \"0\"\n",
    "        \n",
    "        # Saving all the data to a dictionary\n",
    "        news_dict[\"source\"].append(\"lrt.lt\")\n",
    "        news_dict[\"category\"].append(category)\n",
    "        news_dict[\"date\"].append(pub_date)\n",
    "        news_dict[\"title\"].append(title)\n",
    "        news_dict[\"score\"].append(fb_score)\n",
    "        news_dict[\"last_updated\"].append(today_date)\n",
    "    \n",
    "    print(f\"{category} completed! {len(categories)-categories.index(category)-1} categories remaining\")\n",
    "\n",
    "print(\"Data successfully collected!\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting collected information to a pandas dataframe and saving it locally\n",
    "df = pd.DataFrame(news_dict)\n",
    "df.to_csv(\"../Main/Data/data_lrt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a3ba9",
   "metadata": {},
   "source": [
    "## 15min.lt Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42848fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing from which categories to gather the data and assigning a link for them\n",
    "categories_links = {\"lietuvoje\": \"naujienos/aktualu/lietuva\", \"pasaulyje\": \"naujienos/aktualu/pasaulis\",\n",
    "                    \"muzika\": \"kultura/naujienos/muzika\", \"sveikata\": \"gyvenimas/naujienos/sveikata\",\n",
    "                    \"sportas\": \"sportas/naujienos\", \"mokslas-ir-it\": \"verslas/naujienos/mokslas-it\",\n",
    "                    \"kultura\": \"kultura/naujienos\", \"nuomones\": \"naujienos/aktualu/komentarai\",\n",
    "                    \"verslas\": \"verslas/naujienos\", \"gyvenimas\": \"gyvenimas/naujienos\",\n",
    "                    \"eismas\": \"verslas/naujienos/transportas\", \"kriminalai\": \"naujienos/aktualu/nusikaltimaiirnelaimes\",\n",
    "                    \"maistas\": \"maistas/naujienos/naujienos\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32084019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting fields of interest for every article\n",
    "news_dict = {\"source\": [], \"category\": [], \"date\": [], \"title\": [], \"score\": [], \"last_updated\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238da3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.date.strftime(datetime.date.today(), format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4134b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the number of days to collect the articles from\n",
    "days_to_extract = 410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering information of articles for every category (and it's link) in categories_links dictionary\n",
    "for cat, link in categories_links.items():\n",
    "    curr_date = None\n",
    "    prev_day_link = None\n",
    "\n",
    "    # Proceeding through each day for (days_to_extract) number of days\n",
    "    for i in range(days_to_extract):\n",
    "        print(f\"{cat}. Getting {i} day\")\n",
    "        links = []\n",
    "\n",
    "        # Formatting each day and the day before it into suitable form\n",
    "        if not curr_date:\n",
    "            curr_date = datetime.date.strftime(datetime.date.fromisoformat(today_date) + datetime.timedelta(days=1), format=\"%Y-%m-%d\")\n",
    "        else:\n",
    "            curr_date = datetime.date.strftime(datetime.date.fromisoformat(curr_date) - datetime.timedelta(days=1), format=\"%Y-%m-%d\")\n",
    "        prev_date = datetime.date.strftime(datetime.date.fromisoformat(curr_date) - datetime.timedelta(days=1), format=\"%Y-%m-%d\")\n",
    "\n",
    "        # Articles published on DATE and later, can be reached by 'https://www.15min.lt/LINK?offset=DATE 00:00:00' where\n",
    "        # LINK is categorie's link and DATE is the minimal possible date of publishment\n",
    "        \n",
    "        l_prev = f\"https://www.15min.lt/{link}?offset={prev_date} 00:00:00\"\n",
    "        l_curr = f\"https://www.15min.lt/{link}?offset={curr_date} 00:00:00\"\n",
    "\n",
    "        # Articles published on DATE (curr_date) can be collected by getting articles published from DATE-1 (prev_date) and\n",
    "        # later, and removing the articles from it that were published from DATE (curr_date) and later\n",
    "        r = requests.get(l_prev)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        a_group = soup.find(class_=\"visual-list\").select_one(\"div[class^='vl-row vl-row']\")\n",
    "        a = a_group.select_one(\"article[class^='item item-col-']\")\n",
    "        prev_day_link = a.select_one(\".vl-img-container\").get_attribute_list(\"href\")[0]\n",
    "\n",
    "\n",
    "        r = requests.get(l_curr)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Collecting information of each article published on (curr_date)\n",
    "        found_limited_link = False\n",
    "        for a_group in soup.find(class_=\"visual-list\").select(\"div[class^='vl-row vl-row']\"):\n",
    "            if not found_limited_link:\n",
    "                for a in a_group.select(\"article[class^='item item-col-']\"):\n",
    "                    # Getting article's link and checking if it's publishment date is valid\n",
    "                    l = a.select_one(\".vl-img-container\").get_attribute_list(\"href\")[0]\n",
    "                    if l == prev_day_link:\n",
    "                        found_limited_link = True\n",
    "                        break\n",
    "                    links.append(l)\n",
    "                    \n",
    "                    # Getting article's title\n",
    "                    t = (l.split(\"/\")[-1]).split(\"-\")[:-2]\n",
    "                    title = \" \".join(t)\n",
    "                    \n",
    "                    # Getting article's 'facebook likes' collected so far as the score\n",
    "                    try:\n",
    "                        s = a.select_one(\".item-fb-count\")\n",
    "                        fb_score = s.select_one(\".icon-text\").text\n",
    "                    except:\n",
    "                        fb_score = 0\n",
    "                    \n",
    "                    # Saving all the data to a dictionary\n",
    "                    news_dict[\"source\"].append(\"15min.lt\")\n",
    "                    news_dict[\"category\"].append(cat)\n",
    "                    news_dict[\"date\"].append(prev_date)\n",
    "                    news_dict[\"title\"].append(title)\n",
    "                    news_dict[\"score\"].append(fb_score)\n",
    "                    news_dict[\"last_updated\"].append(today_date)\n",
    "            else:\n",
    "                break\n",
    "    print(f\"{cat} completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1818c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting collected information to a pandas dataframe and saving it locally\n",
    "df = pd.DataFrame(news_dict)\n",
    "df.to_csv(\"../Main/Data/data_15min.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
